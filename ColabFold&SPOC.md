## Important Links

The repo for ColabFold in your local PC is here: https://github.com/YoshitakaMo/localcolabfold

The tutorial for running ColabFold notebook on a HPC over SSH is here: https://www.jameslingford.com/blog/colabfold-hpc-ssh-howto/

The repo for SPOC is here: https://github.com/walterlab-HMS/SPOC

# Run ColabFold

## What is ColabFold and how is it compared to alphafold/2.3.1
ColabFold
- Optimized version of AlphaFold (developed by Mirdita et al.)
- Replaces JackHMMER with MMseqs2 (ultra-fast multiple sequence alignments, MSA).
- Optional smaller databases (e.g., ColabFoldDB) — fits on a laptop/server (~100 GB).
- 5–10x faster inference time.
- Slight tweaks to internal parameters for better multimer handling.
- Slightly lower MSA depth compared to full AlphaFold, but not noticeable for most proteins.
- Supports batch prediction, faster amber relaxation, etc.

AlphaFold 2.3.1
- Original DeepMind release.
- JackHMMER search → very slow but very deep MSAs.
- Requires huge databases like BFD, Uniref90, MGnify (over 2 TB total).
- Very accurate especially for orphan proteins (no homologs).
- Running time: hours per complex if the sequence is long.

## What is the difference between local ColabFold and ColabFold?
ColabFold is a cloud-based version of the AlphaFold model that runs on Google Colab. It provides a convenient way to use AlphaFold without needing to set up the environment locally.

Local ColabFold is essentially the same software as ColabFold, but it's installed and run locally on your system or an HPC cluster. You set it up manually, often in a Python environment with dependencies. This is the one for HPC.

### Create .sh file: ``` nano protein1_protein2.sh ```

### Copy and paste the following script in:

    This code will submit the the same number of jobs to gpu as to the number of predictions you want to make. You can find the colabfold ouput in the same folder as the fasta sequence file. 
    
    Example script:
    
    ```bash
    #!/bin/bash
    #BSUB -q gpu
    #BSUB -R "rusage[mem=20G]"
    #BSUB -J "predict[1-1293]"
    #BSUB -gpu "num=1:mode=exclusive_process:j_exclusive=yes"
    #BSUB -n 1
    #BSUB -W 2:00
     
     
    # Get the list of input folders (stored beforehand)
    INPUT_LIST="input_folders.txt"
    FOLDER=$(sed -n "${LSB_JOBINDEX}p" "$INPUT_LIST")
     
    # Set output file base name
    FASTA=$(find "$FOLDER" -maxdepth 1 -name "*.fasta" | head -n 1)
    BASENAME=$(basename "$FASTA" .fasta)
     
    # Redirect output to folder-specific logs
    exec > "$FOLDER/${BASENAME}.out" 2> "$FOLDER/${BASENAME}.err"
     
    # Load module
    module load localcolabfold/1.5.5
    LOCALCOLABIMG=/share/pkg/containers/localcolabfold/localcolabfold_1.5.5.sif
     
    # Run prediction
    singularity exec --nv $LOCALCOLABIMG colabfold_batch \
         --templates --num-recycle 3 --num-ensemble 1 --num-models 3 "$FASTA" "$FOLDER"
    ```
    - Running 3 models because that's what spoc were training on.
---
# SPOC Set-up
## What is SPOC and why we use it?

### Clone the SPOC repository
- To clone the SPOC repository, use the following command:
  ``` bash
  git clone https://github.com/walterlab-HMS/SPOC.git
  ```
- Navigate into the cloned directory: ```cd SPOC```

### Create environment to load necessary dependencies

If you are using conda or miniconda, please refer to original repo. The following is for micromamba

- create the environment:
  ```bash
  micromamba env create -f SPOC/environment.yml
  ```
- activate the environment:
  ``` bash
  micromamba activate spoc_venv
  ```

### Run SPOC 
Here is an example input folder:
```bash
my_afm_predictions_folder/
│-- DONS_HUMAN__MCM3_HUMAN__1374aa.a3m.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_001_alphafold2_multimer_v3_model_1_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_002_alphafold2_multimer_v3_model_2_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_003_alphafold2_multimer_v3_model_3_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_000.pdb.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_002_alphafold2_multimer_v3_model_2_seed_000.pdb.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_003_alphafold2_multimer_v3_model_3_seed_000.pdb.xz

```
- contains a file with .a3m (which is same for all the model)
- three .json files and three .pdb files generated by colabfold
- Run the prediction by
```bash
python3 run.py my_afm_predictions_folder
```

---
# Run Batch files for colabfold and SPOC
## Colabfold
### Example Script for a low number of tasks at once (<20):
```bash
    #!/bin/bash
    # Go to the folder that contains all the folders for fasta file
    cd All_multimer
    
    # Loop through each folder containing the FASTA file
    for dir in */; do
        # Remove trailing slash to get folder name
        folder=${dir%/}
    
        # Find the fasta file inside the folder
        fasta=$(find "$folder" -maxdepth 1 -name "*.fasta" | head -n 1)
    
        # Skip if no fasta file found
        if [[ -z "$fasta" ]]; then
            echo "No FASTA in $folder, skipping..."
            continue
        fi
    
        # Set job and output file base names based on folder
        base_name=$(basename "$fasta" .fasta)
    
        # Generate a temporary job script
        job_script="${folder}/run_colabfold.bsub"
    
        cat > "$job_script" <<EOF
    #!/bin/bash
    #BSUB -q gpu
    #BSUB -R "rusage[mem=20G]"
    #BSUB -J ${base_name}_models
    #BSUB -gpu "num=1"
    #BSUB -n 1
    #BSUB -W 2:00
    #BSUB -oo ${folder}/${base_name}.out
    #BSUB -eo ${folder}/${base_name}.err
    
    module load localcolabfold/1.5.5
    LOCALCOLABIMG=/share/pkg/containers/localcolabfold/localcolabfold-1.5.5.sif
    
    singularity exec --nv $LOCALCOLABIMG colabfold_batch \
         --templates --num-recycle 3 --num-ensemble 1 --num-models 3 "$fasta" "${folder}"
    
    EOF
    
        # Submit the job
        bsub < "$job_script"
    done

```
### Example Script for a large number of tasks, e.g., ~2000
- Step 1: Make sure you have the run_wrapper.py in the same directory as the run.py

  Create a new sh file within the SPOC folder: ```nano run_wrapper.py```

  Copy and paste the following script into the file:

  This code will create a temporary directory to only take the a3m, .json, and .pdb files from the colabfold output and runs run.py on thie temporary directory
    ```bash
    # run_wrapper.py
    import os
    import sys
    import glob
    import subprocess
    import tempfile
    import re
    def run_prediction(input_folder):
        input_folder = os.path.abspath(input_folder)
        folder_name = os.path.basename(input_folder)
        # Match only top-level files
        a3m_files = glob.glob(os.path.join(input_folder, '*.a3m'))
        json_files = sorted(glob.glob(os.path.join(input_folder, '*_scores_rank_*_model_*_seed_000.json')))
        pdb_files = sorted(glob.glob(os.path.join(input_folder, '*_unrelaxed_rank_*_model_*_seed_000.pdb')))
        all_files = a3m_files + json_files + pdb_files
    
        if not all_files:
            print(f"[{input_folder}] No valid input files found.")
            return
    
        # Temporary clean directory with only relevant files
        with tempfile.TemporaryDirectory() as tmpdir:
            for file_path in all_files:
                link_path = os.path.join(tmpdir, os.path.basename(file_path))
                os.symlink(os.path.abspath(file_path), link_path)
    
            print(f"[{input_folder}] Running prediction...")
            output_file = f"{folder_name}_SPOC_output.csv"
            subprocess.run(["python3", "run.py", tmpdir,"--output",output_file], cwd=os.path.dirname(__file__), check=True)
    
    if __name__ == "__main__":
        if len(sys.argv) < 2:
            print("Usage: python run_wrapper.py <folder1> [<folder2> ...]")
            sys.exit(1)
    
        for folder in sys.argv[1:]:
            run_prediction(folder)
    ```
- Step 2: In the same directory of your All_Multimer folder, get all the names of the fasta files into the input_folders.txt
    ```bash
    find All_multimer -maxdepth 1 -type d -not -path 'All_multimer' > input_folders.txt
    ```

- Step 3: Create a file at the same directory as the input_folders.txt:```nano run_prediction.sh```

    Copy and paste into the file (be sure to change the directory in the script!):
    ```bash
    #!/bin/bash
    #BSUB -q large
    #BSUB -n 40
    #BSUB -J spoc_predict
    #BSUB -W 96:00
    #BSUB -oo spoc_predict.out
    #BSUB -eo spoc_predict.err
    
    # Setup micromamba
    eval "$(micromamba shell hook --shell=bash)"
    micromamba activate spoc_venv
    
    # Define the path to run_wrapper.py
    RUN_WRAPPER_PATH="/home/fangyi.zhai-umw/colabfold/SPOC/run_wrapper.py"
    
    # Go to the directory that contains all the predictions
    cd /home/fangyi.zhai-umw/colabfold/All_MNK1
    
    for folder in */; do
        folder=${folder%/}
        echo "Running on $folder"
        python3 "$RUN_WRAPPER_PATH" "$folder"
    done
    ```
    Now you have a list of SPOC predictions within the same directory as the run.py (which is your SPOC folder)
